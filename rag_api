from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Optional
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage
import uvicorn
import re
import os
from concurrent.futures import ThreadPoolExecutor, TimeoutError

# === Setup ===
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "sk-proj-xe0Qwe6xjMh0ENu8jKCpQY9H5h6gpLdCYAiGxpi4oxOTBKyPa1zReRCAWbBwFQKEJKWDOnE4D4F5MrMfWtTHZmnBeJ8slrIA")
embedding_model = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
llm = ChatOpenAI(model="gpt-3.5-turbo", openai_api_key=OPENAI_API_KEY)

CHARACTER_MAP = {
    0: {"name": "Anwar Sadat", "db": "sadat_db"},
    1: {"name": "Nefertiti", "db": "nefertiti_db"},
    2: {"name": "Akhenaten", "db": "akhenaten_db"},
    3: {"name": "Tutankhamun", "db": "tutankhamun_db"},
    4: {"name": "Ramses II", "db": "ramses_db"},
}

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# === Models ===
class AskRequest(BaseModel):
    username: str
    characterIndex: int
    question: str
    summary: Optional[str] = None

class SummarizeRequest(BaseModel):
    username: str
    characterIndex: int
    previous_summary: Optional[str] = None
    question: str
    answer: str

# === Utility Functions ===

def summarize_summary_with_qa(previous_summary: Optional[str], question: str, answer: str, username: str, characterIndex: int) -> str:
    character_data = CHARACTER_MAP[characterIndex]
    character_name = character_data["name"]
    system_prompt = SystemMessage(
        content="You are an assistant that summarizes conversations between a user and a historical character. Combine the previous summary and the new Q&A pair into a single concise summary."
    )

    summary_input = f"""Previous Summary: {previous_summary or "None"}

New QA Pair:
{username}: {question}
{character_name}: {answer}

Return the updated summary:"""

    updated_summary = llm.invoke([system_prompt, {"role": "user", "content": summary_input}])
    return updated_summary.content.strip()

def perform_vector_search(db, search_input):
    return db.similarity_search_with_relevance_scores(search_input, k=3)

# === Ask Endpoint ===

@app.post("/ask")
def ask_question(payload: AskRequest):
    username = payload.username
    index = payload.characterIndex
    question = payload.question.strip()
    summary = payload.summary.strip() if payload.summary else None

    if index not in CHARACTER_MAP:
        return {"error": "Invalid character index."}

    character_data = CHARACTER_MAP[index]
    character_name = character_data["name"]
    db_path = character_data["db"]

    # === Step 1: RAG with summary + question ===
    db = Chroma(persist_directory=db_path, embedding_function=embedding_model)
    
    # Combine summary with question if summary exists
    search_input = f"{summary}\n\n{question}" if summary else question

    # Set up timeout for vector search
    executor = ThreadPoolExecutor(max_workers=1)
    future = executor.submit(perform_vector_search, db, search_input)
    
    try:
        current_docs = future.result(timeout=4)  # 7 seconds timeout
        primary_threshold = 0.4
        good_docs = [doc for doc, score in current_docs if score >= primary_threshold]
        context = "\n\n".join([doc.page_content for doc in good_docs]) if good_docs else ""
        source_info = "RAG used" if good_docs else "No documents passed threshold"
    except TimeoutError:
        context = ""
        source_info = "Vector search timed out, using direct response"
    except Exception as e:
        context = ""
        source_info = f"Error in vector search: {str(e)}, using direct response"

    # === Step 2: System + Summary ===
    system_prompt = {
        "role": "system",
        "content": f"""You are {character_name} speaking to a modern-day tourist called {username}.
Your only emotions are either (happy), (sad), (greet), or (angry) at the end of the sentence depending on the context. 
You respond as yourself, not as an AI, and keep your tone elegant and relevant. You don't know anyone after your time."""
    }

    messages = [system_prompt]

    if summary:
        messages.append({
            "role": "system",
            "content": f"Summary of previous conversation: {summary}"
        })

    user_prompt = f"References:\n{context}\n\nCurrent Question:\n{question}" if context else question
    messages.append({"role": "user", "content": user_prompt})

    response = llm.invoke(messages)
    answer = response.content

    # === Step 3: Extract Emotion ===
    emotion_match = re.search(r"(happy|sad|angry|greet)", answer, re.IGNORECASE)
    emotion = emotion_match.group(1).lower() if emotion_match else "unknown"

    print("---- INTERACTION LOG ----")
    print(f"User: {username}")
    print(f"Character: {character_name}")
    print(f"Question: {question}")
    print(f"Answer: {answer} \n")
    print(f"Emotion: {emotion} \n")
    print(f"Used RAG: {'Yes' if context else 'No'}")
    if context:
        print("Documents Used:")
        for doc in good_docs:
            print(doc.page_content)
    print("--------------------------")


    return {
        "username": username,
        "character": character_name,
        "question": question,
        "answer": answer,
        "emotion": emotion,
        "source": source_info
    }

# === Summarize Endpoint ===

@app.post("/summarize")
def summarize(payload: SummarizeRequest):
    updated_summary = summarize_summary_with_qa(payload.previous_summary, payload.question, payload.answer, payload.username, payload.characterIndex)
    return {
        "updated_summary": updated_summary
    }

# === Run Server ===
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
